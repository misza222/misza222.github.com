{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Why use F.cross_entropy?\"\n",
    "description: \"... instead of computing it yourself?\"\n",
    "format: html\n",
    "date: \"1/12/2023\"\n",
    "categories: training\n",
    "image: 1163388725_water_trickling_down_the_mountain_photorealistic_4k.png\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/misza222/misza222.github.com/blob/main/posts/f_cross_entropy/index.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apart from less code of course! Another reason: it is safer!\n",
    "\n",
    "I like to have more controll over what and how I am doing things, instead of using black boxes. But be warned that you can get burned when computing negative log likelihood yourself (the same is tru for softmaxes for example).\n",
    "\n",
    "see this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., nan]), tensor(nan))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor([-100, -5, 2, 100])\n",
    "logits = logits.exp()\n",
    "probs = logits / logits.sum()\n",
    "probs, probs.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makes sense, right? exp(100) is VERY large, so if your network misbehaves and produces extreme activations, you have a problem, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000e+00, 0.0000e+00, 2.7465e-43, 1.0000e+00]), tensor(1.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor([-100, -5, 2, 100])\n",
    "\n",
    "# here we deduct max value from the logits, so everyting is in (-âˆž, 0)\n",
    "#----------------------\n",
    "logits -= logits.max() \n",
    "#----------------------\n",
    "\n",
    "logits = logits.exp()\n",
    "probs = logits / logits.sum()\n",
    "probs, probs.sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is working nicely, and that's what F.cross_entropy does internally. Of course, you can always add that normalization to safeguard against such cases (or add `batchnorm` layers to your architecture if you don't wan't to bother about such cases at the cost of a little more complexity and state in your model).\n",
    "\n",
    "Plus of course I am sure there are also more good computational efficiency reasons to use torch'es built-in method do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdaea57c0b9010ea74c77e884f0f49a414be4bd0fcb5deeb3a8fb91359695020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
