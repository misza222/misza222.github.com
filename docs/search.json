[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Technical notes",
    "section": "",
    "text": "Generate synthetic dialogues programatically for any topic\n\n\n\n\n\n\nllm, design, ideation\n\n\n\nLearn Any Business Domain Through Conversations\n\n\n\n\n\nJun 1, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nBest way to do traditional NLP?\n\n\nGLiNER2 and it‚Äôs example usage\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\nFeb 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nNeRF basics\n\n\n\n\n\n\nnerf\n\n\n\ncompanion post for DSS 2023 conference\n\n\n\n\n\nNov 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTraining vision models on synthetic images\n\n\n\n\n\n\npaper review\n\nvision\n\n\n\nNot SOTA performance, but quite good\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhy use F.cross_entropy?\n\n\n\n\n\n\ntraining\n\n\n\n‚Ä¶ instead of computing it yourself?\n\n\n\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOptuna is hot on Kaggle\n\n\n\n\n\n\ntraining\n\n\n\nHow to use it with code examples\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to read papers?\n\n\n\n\n\n\npaper review\n\n\n\nGreat advice from Andrew Ng\n\n\n\n\n\nDec 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMath resources\n\n\n\n\n\n\nmath\n\n\n\nAll you need to within weeks understand Deep Learning and be reasonably comfortable with papers.\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "example_posts/post-with-code/index.html",
    "href": "example_posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/math_resources/index.html",
    "href": "posts/math_resources/index.html",
    "title": "Math resources",
    "section": "",
    "text": "My Uni days are long gone, I was reasonably good at math, but over the years of not using it, I almost feel like the knowledge was never there.\nHere is my journey through the process of remembering some math that I need to feel more comfortable with for the basics of deep learning and to be able to digest papers in the broad area of deep learning research.\nAdvice to my former self: First read some papers, struggle through them, let the frustration build up so you have a motivation to learn + you will also build an intuition of what tools you may actually need!\nAs for the resouces, I started with Deep Learninig book few years ago, but got discouraged by the theory and I didn‚Äôt have enough practice to know that it will be useful some day.\nRecently I found out that there is a growing comunity of people around that book led by Sanyam Buthani. Apart from the community and help + motivation to learn that comes with it, there are number of resources for self study, such as notebooks with all the concepts translated into code, which makes it so much more practical.\nStaying around the same book, there are great lectures going through each chapter of that book.\nIf you want to learn more about torch.autograd, frameworks that do automatic differentiation in general and understand calculus that is the engine of deep learning machine there is an excelent The Matrix Calculus You Need For Deep Learning.\nAnother interesting resources are Mathematics for Machine Learning and short lectures covering pretty much all you need for the topic in a very accessible, visual and short form.\nLast but not least, Andrew Ng‚Äôs advice on how to read a paper which I also sumarize here!\nAll these resources are freely available online."
  },
  {
    "objectID": "posts/how_to_read_papers/index.html",
    "href": "posts/how_to_read_papers/index.html",
    "title": "How to read papers?",
    "section": "",
    "text": "Andrew Ng gave an career advice lecture at Stanford in 2019 where he also mentioned how to read academic papers.\nRead it taking multiple passes through the paper:\n\nTitle + Abstract + Figures\nIntro + Conclusions + Figures + Skim rest\nRead text but skip math\nRead all of it but skip what doesn‚Äôt make sense\n\nwhich boils down to:\nGo from efficient and high information content first and dig into the harder bits gradually\nSome of the questions to keep in mind during the process (and to decide whether to go to the next step):\n\nWhat did authors try to acomplish?\nWhat are the key elements of the approach?\nWhat can you use yourself?\nWhat other references do you want to follow?\n\nIt is worth watching, as it also sumarizes the process of creating and reviewing the paper to give rationale for that process."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Husband, father, Christian, programmer, fascinated with NNs and most recently with Computer Vision, specifically NeRFs.\nI took a sabbatical to explore machine/deep learning but I am open to offers that will lead me into direction of greater enlightement in a field of Deep Learning, CG and applications in Med/Pharma. Can be fully Remote or in Warsaw/Poland.\nI write this blog mostly for:\n\nmyself to better remember what I learn (and I have tons of blog posts that I never publish as no time to polish it for other to learn from)\nothers to learn from - I often post about stuff that I learn with others (such as HF RL course topics) - it is easier to convince yourself to write good quality content with that objective in mind :)\nrectuiters/employers - so they can discover what I learn and what I enjoy doing\n\nIf you are on a learning path like me, I can‚Äôtrecommend enough writing a blog.\nIf you are interested in my careed so far see Curriculum Vitae\nMost images dreamt by Stable Diffusion and DALL¬∑E 3, hosted on github pages, rendered with Quarto."
  },
  {
    "objectID": "posts/optuna_notes/index.html",
    "href": "posts/optuna_notes/index.html",
    "title": "Optuna is hot on Kaggle",
    "section": "",
    "text": "While doing HF RL course I bumped into Optuna, then I‚Äôve noticed that ppl do it on Kaggle a lot - this is like a badge of honor for a package if it is being used on a top ML competition site ergo it is worth learning!\nSome vocab to get started:\n\nobjective(trial) function - function to optimize\ntrial - a single test, also an object passed to the objective function\nstudy - a set of trial at the end of which you get a suggestion of parameters to use\nparameter - parameter to optimize\nsetting initial values for parameters to optimize:\n\noptuna.trial.Trial.suggest_categeorical(‚Äòname‚Äô, [‚Äòlist‚Äô])\noptuna.trial.Trial.suggest_int(‚Äòname‚Äô, min, max)\noptuna.trial.Trial.suggest_float(‚Äòname‚Äô, min, max)\n\n\nHere is a quick summary of how to use it:\n\n\nCode\n%pip install optuna\n\n\n\nimport optuna\n# to supress unnecessary output as it prints quite a lot by default\noptuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# Task: with 100 trials find a minimum for a function (x-10)**2\n\n# objective function to minimize\ndef objective(trial):\n    # this is just returning float and internally in the trial optuna \n    # keeps track of all the values used\n    x = trial.suggest_float(\"x\", -100, 100) \n    return (x - 10)**2\n\n# create optimization object that will keep track of the whole process\nstudy = optuna.create_study() \n\n# and run optimization with 100 runs\nstudy.optimize(objective, n_trials=100)\n\n# get the optimized values\nstudy.best_params['x'] # we are pretty close\n\n10.108796067394927\n\n\n\n# however it won't do magic if you don't give it enough \"space\"\n# here if you give it just 10 trials, it will usually miss quite \n# substantially\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -100, 100)\n    return (x - 10)**2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\n\nstudy.best_params['x'] # ... it is usually not so good\n\n-10.114148942248292\n\n\nhow optuna works internally is quite simple but ingenious: each call to trial.suggest_*() function already returns a python variable, so you can use it in your code straight away:\n\nstudy = optuna.create_study()\ndef objective(trial):\n    # trial.suggest_int returns integer - all the magic of storing\n    # what value was used in a specific trial is recorded in trial object\n    i = trial.suggest_int('x', 0, 100, step=10)\n    print(f\"next {i=}\")\n    return i\n\nstudy.optimize(objective, n_trials=10)\nstudy.best_params['x']\n\nnext i=0\nnext i=0\nnext i=90\nnext i=60\nnext i=70\nnext i=10\nnext i=10\nnext i=10\nnext i=60\nnext i=90\n\n\n0\n\n\nIt is interesting at first how the numbers are drawn from the space - this is all quasi random and duplicates are possible. Especially if we have very limited space of available unique values as in here. This is not an implementation bug - here we deal with a single variable, but if we have multiple ones, it quite makes sense to try simillar values if we variate other parameters at the same time. This is default, but you can choose different strategires for drawing values.\n\n\n\n\n\n# however with 10 trials...\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -100, 100)\n    return (x - 10)**2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"After 10 trials we got {study.best_params['x']=:.2f}\") # ... it is usually not so good\n\n# but training for another 10 iterations does the trick\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"... but 10 more runs get us closer {study.best_params['x']=:.2f}\") # ok, now it is better :)\n\nAfter 10 trials we got study.best_params['x']=1.35\n... but 10 more runs get us closer study.best_params['x']=9.38\n\n\n\n\n\nOr just run hiperparameter searches when your colab disconnects\nOptuna allows for distrubuted trials\n\n#straight from optuna docs @ https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10)\n    return (x - 2) ** 2\n\n\nif __name__ == \"__main__\":\n    study = optuna.load_study(\n        study_name=\"distributed-example\", storage=\"mysql://root@localhost/example\"\n    )\n    study.optimize(objective, n_trials=100)\n\n\n\n\nIt was a bit tricky to understand for me how it works, as it is usually hidden in handlers to specific libraries. But here is a clear example that doesn‚Äôt hide anything from you:\n\noptuna.logging.set_verbosity(optuna.logging.INFO)\nimport random\n\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10) # you draw next value\n\n    # and this is an inner loop simulating inner loop in the \n    # optimization functions, like going through batches in\n    # NN training\n    for i in reversed(range(10)): \n        # here we just make up a number simulating intermediate result\n        # that is sent to optuna to validate if it is worth continuing\n        made_up_intermediate_value = random.randint(1, 10)\n        # it is reported to optuna\n        trial.report(made_up_intermediate_value, i)\n\n        # Handle pruning based on the intermediate value.\n        if trial.should_prune(): # Optuna suggests to prune?\n            print(f'''\\\nPruning trial {trial.number} with value {made_up_intermediate_value=}\\n\\\nbecause it is already less optimal than previously recorded best value''', flush=True, end='')\n            # if yes we throw exception that is handled by `optimize` method\n            # in optuna\n            raise optuna.TrialPruned() \n\n\n\n    return (x - 2) ** 2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\nstudy.best_params['x']\n\n\n[I 2023-01-13 13:49:05,151] A new study created in memory with name: no-name-826daad7-a560-4139-8012-384a08aecae9\n\n[I 2023-01-13 13:49:05,154] Trial 0 finished with value: 27.811950820406743 and parameters: {'x': 7.2737037099562905}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,157] Trial 1 finished with value: 81.56034995467249 and parameters: {'x': -7.0310768989458}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,162] Trial 2 finished with value: 1.8078821300442816 and parameters: {'x': 3.3445750741569924}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,165] Trial 3 finished with value: 51.599347421072665 and parameters: {'x': 9.183268575034116}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,168] Trial 4 finished with value: 87.26649837508509 and parameters: {'x': -7.341653942160622}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 5 with value made_up_intermediate_value=10\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,170] Trial 5 pruned. \n\n[I 2023-01-13 13:49:05,181] Trial 6 finished with value: 26.47065704510398 and parameters: {'x': -3.1449642413824392}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,185] Trial 7 finished with value: 29.163269432320956 and parameters: {'x': 7.400302716729957}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 8 with value made_up_intermediate_value=6\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,187] Trial 8 pruned. \n\n[I 2023-01-13 13:49:05,194] Trial 9 finished with value: 38.838470030340275 and parameters: {'x': 8.232051831486984}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\n3.3445750741569924"
  },
  {
    "objectID": "posts/optuna_notes/index.html#useful-tricks",
    "href": "posts/optuna_notes/index.html#useful-tricks",
    "title": "Optuna is hot on Kaggle",
    "section": "",
    "text": "# however with 10 trials...\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -100, 100)\n    return (x - 10)**2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"After 10 trials we got {study.best_params['x']=:.2f}\") # ... it is usually not so good\n\n# but training for another 10 iterations does the trick\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"... but 10 more runs get us closer {study.best_params['x']=:.2f}\") # ok, now it is better :)\n\nAfter 10 trials we got study.best_params['x']=1.35\n... but 10 more runs get us closer study.best_params['x']=9.38\n\n\n\n\n\nOr just run hiperparameter searches when your colab disconnects\nOptuna allows for distrubuted trials\n\n#straight from optuna docs @ https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10)\n    return (x - 2) ** 2\n\n\nif __name__ == \"__main__\":\n    study = optuna.load_study(\n        study_name=\"distributed-example\", storage=\"mysql://root@localhost/example\"\n    )\n    study.optimize(objective, n_trials=100)\n\n\n\n\nIt was a bit tricky to understand for me how it works, as it is usually hidden in handlers to specific libraries. But here is a clear example that doesn‚Äôt hide anything from you:\n\noptuna.logging.set_verbosity(optuna.logging.INFO)\nimport random\n\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10) # you draw next value\n\n    # and this is an inner loop simulating inner loop in the \n    # optimization functions, like going through batches in\n    # NN training\n    for i in reversed(range(10)): \n        # here we just make up a number simulating intermediate result\n        # that is sent to optuna to validate if it is worth continuing\n        made_up_intermediate_value = random.randint(1, 10)\n        # it is reported to optuna\n        trial.report(made_up_intermediate_value, i)\n\n        # Handle pruning based on the intermediate value.\n        if trial.should_prune(): # Optuna suggests to prune?\n            print(f'''\\\nPruning trial {trial.number} with value {made_up_intermediate_value=}\\n\\\nbecause it is already less optimal than previously recorded best value''', flush=True, end='')\n            # if yes we throw exception that is handled by `optimize` method\n            # in optuna\n            raise optuna.TrialPruned() \n\n\n\n    return (x - 2) ** 2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\nstudy.best_params['x']\n\n\n[I 2023-01-13 13:49:05,151] A new study created in memory with name: no-name-826daad7-a560-4139-8012-384a08aecae9\n\n[I 2023-01-13 13:49:05,154] Trial 0 finished with value: 27.811950820406743 and parameters: {'x': 7.2737037099562905}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,157] Trial 1 finished with value: 81.56034995467249 and parameters: {'x': -7.0310768989458}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,162] Trial 2 finished with value: 1.8078821300442816 and parameters: {'x': 3.3445750741569924}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,165] Trial 3 finished with value: 51.599347421072665 and parameters: {'x': 9.183268575034116}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,168] Trial 4 finished with value: 87.26649837508509 and parameters: {'x': -7.341653942160622}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 5 with value made_up_intermediate_value=10\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,170] Trial 5 pruned. \n\n[I 2023-01-13 13:49:05,181] Trial 6 finished with value: 26.47065704510398 and parameters: {'x': -3.1449642413824392}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,185] Trial 7 finished with value: 29.163269432320956 and parameters: {'x': 7.400302716729957}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 8 with value made_up_intermediate_value=6\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,187] Trial 8 pruned. \n\n[I 2023-01-13 13:49:05,194] Trial 9 finished with value: 38.838470030340275 and parameters: {'x': 8.232051831486984}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\n3.3445750741569924"
  },
  {
    "objectID": "posts/f_cross_entropy/index.html",
    "href": "posts/f_cross_entropy/index.html",
    "title": "Why use F.cross_entropy?",
    "section": "",
    "text": "apart from less code of course! Another reason: it is safer!\nI like to have more controll over what and how I am doing things, instead of using black boxes. But be warned that you can get burned when computing negative log likelihood yourself (the same is tru for softmaxes for example).\nsee this example:\n\nimport torch\n\nlogits = torch.tensor([-100, -5, 2, 100])\nlogits = logits.exp()\nprobs = logits / logits.sum()\nprobs, probs.sum()\n\n(tensor([0., 0., 0., nan]), tensor(nan))\n\n\nmakes sense, right? exp(100) is VERY large, so if your network misbehaves and produces extreme activations, you have a problem, but‚Ä¶\n\nimport torch\n\nlogits = torch.tensor([-100, -5, 2, 100])\n\n# here we deduct max value from the logits, so everyting is in (-‚àû, 0)\n#----------------------\nlogits -= logits.max() \n#----------------------\n\nlogits = logits.exp()\nprobs = logits / logits.sum()\nprobs, probs.sum()\n\n(tensor([0.0000e+00, 0.0000e+00, 2.7465e-43, 1.0000e+00]), tensor(1.))\n\n\nis working nicely, and that‚Äôs what F.cross_entropy does internally. Of course, you can always add that normalization to safeguard against such cases (or add batchnorm layers to your architecture if you don‚Äôt wan‚Äôt to bother about such cases at the cost of a little more complexity and state in your model).\nPlus of course I am sure there are also more good computational efficiency reasons to use torch‚Äôes built-in method do that."
  },
  {
    "objectID": "posts/nerf_basics/index.html",
    "href": "posts/nerf_basics/index.html",
    "title": "NeRF basics",
    "section": "",
    "text": "This is a companion blog post for my presentation given at Data Science Summit 2023 in Warsaw.\nThe presentation itself is on google docs  and recording of the presentation will be published later.\n\n\nRight now I have not much to add to the presentation. But make sure to check out extensive speaker notes there.\nBelow is the dozer that I used as a training data for nerf.studio that is rendered here with poly.cam and lumalabs.ai"
  },
  {
    "objectID": "posts/synthetic_vision_training/index.html",
    "href": "posts/synthetic_vision_training/index.html",
    "title": "Training vision models on synthetic images",
    "section": "",
    "text": "What if we‚Äôll be able to train vision NN model with 0, nil, nada real world examples? But we have to train on something, right? Surely training on a pure noise won‚Äôt give us anything useful? See CNN-Rand or StyleGAN-Random (both initialized randomly, no training whatsoever) below and you will be surprised! Moreover if we construct synthetic images that are closer to the real world images, can we train on it with some positive outcomes? But then how well can we train? Paper presented below tried to answer that question.\nUnless tagged otherwise, ideas in this blog post come from the Learning to See by Looking at Noise paper, recorded presentation by Antonio Torralba who is one of the authors and presentation by Rosanne Liu during Deep Learning: Classics and Trends.\n\nRationale for this exercise\nModels are more and more reliant on data. CLIP need 400 000 000 images to be trained well for example. What if we could build a synthetic dataset to train? Why:\n\nyou don‚Äôt have access to data\ncheaper to maintain the data\nmaybe generating a good synthetic dataset can be better than real data (no human bias for example)\n\n\n\nTask\nThe training objective is classification of ImageNet-100 images.\n\n\nTraining procedure\nIt is done in 2 stages:\n\nThe ‚Äúbase‚Äù network is trained using unsupervised contrastive learning (simplifying it is done by identifying if images are the same or come from the same source image with transformations applied; details of the specific approach used in the paper)\nFinal layer (but could be layers I think) are trained briefly on actual data to create a head of the model (to me it was not clear reading the paper but see this training script by authors)\n\nTechnicalities derived from the code:\n\nunsupervised part (1):\n\nmodel parameters: TBD\nepochs: 200\ntime: TBD (most expensive part)\n\nsupervised part (2):\n\nmodel parameters: TBD\nepochs: 100\ntime: TBD (but as it is training just single fc layer, this will be cheap)\n\n\n\n\nDatasets\nSee this image of datasets used in this experiment which are referred blow on the benchmark graph.\n\n\n\nResults\n\nBlack bars are different baselines and coloured bars represent various sythetic datasets. About 20% is a difference between the best model trained on actual images and best model pre-trained on synthetic images. The surprising bit is, that randomly initialized networks can give much better than chance results (and random choice is 1% as we have 100 classes): CNN - Rand and StyleGAN - Random. The explanation was, that last fully connected layer will give some performance boost, but Antonio in his video also mentions that even some of those randomly initialized ‚Äúfeatures must be useful to some degree‚Äù. So for example in CNN some of the filters extract information that is then used by linear layer to reason upon.\n\n\nMy Conclusions\nCan this approach democratize access to data, as currently data collection and maintenance is being more and more concentrated? It looks like it, but for now it comes at the cost of performance. There was also a lot of laughter during the Antonios presentation about Stable Diffusion, so I have to add that: what would be the result if data was generated by SD model?"
  },
  {
    "objectID": "example_posts/welcome/index.html",
    "href": "example_posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/gliner2/index.html",
    "href": "posts/gliner2/index.html",
    "title": "Best way to do traditional NLP?",
    "section": "",
    "text": "We use LLMs for everything nowadays, from adding numbers, to finding a meaning of life. But no, it is not another post about Large Language Models as they are often not optimal (accuracy, reliability, environment, cost)-wise? But it is about good old NLP and about what it seems like use-cases from previous century, but still very usefull and with positive ROI.\nBeing consumed by LLMs, agents and stuff, I accidentally bumped into transformers based information extraction system and as an exercise tried it to replace some of my nlp code for NER and classification to use GLiNER2:\n\nExtract entities, classify text, parse structured data, and extract relations‚Äîall in one efficient model.\n\nTL;DR: the result was shorter code, faster inference, and a much cleaner pipeline.\nSo what I was trying to do:\n\nextract some names and concepts,\nclassify documents with custom labels\n\n‚Ä¶ but just look at the code and benchmarks\n\nTraditional NER and zero shot classification\n\n# !pip install spacy transformers numpy torch\n\n\n# !spacy download en_core_web_lg\n\n\n# Load NLP\nimport spacy\nfrom transformers import pipeline\nimport re\nimport numpy as np\nnlp = spacy.load(\"en_core_web_lg\")\nclassifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\"\n)\n\n\n\n\n\ndocs_to_classify = [\n    \"John Smith joined Acme Corp in Jan 2021 and worked there until late 2025 as a senior engineer based in Berlin.\",\n    \"It is always sunny in Filadelphia\",\n    \"For us, Anonumoys Inc. based in Warsaw, Poland, traditional NLP is more predictible then LLMs, at least in February of 2026\",\n]\n\n\ndef classical_nlp(text:str) -&gt; dict:\n    # NER\n    doc = nlp(text)\n\n    entities = {\n        \"PERSON\": [],\n        \"ORG\": [],\n        \"GPE\": [],\n        \"DATE\": []\n    }\n    \n    for ent in doc.ents:\n        if ent.label_ in entities:\n            entities[ent.label_].append(ent.text)\n    \n    # Classification\n    labels = [\"employment\", \"weather\", \"other\"]\n    clf = classifier(text, candidate_labels=labels)\n    \n    doc_type = clf[\"labels\"][np.argmax(clf[\"scores\"])]\n\n    # glue it together\n    return {\n        \"entities\": {\n        \"person\": entities[\"PERSON\"],\n        \"company\": entities[\"ORG\"],\n        \"location\": entities[\"GPE\"],\n        \"date\": entities[\"DATE\"],\n        },\n        \"category\": doc_type\n    }\n\n\n\n‚Ä¶ same with GLiNER2\n\n# !pip install gliner2\n\n\n# Load GLiNER\nfrom gliner2 import GLiNER2\nmodel = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")\n\nYou are using a model of type extractor to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n\n\n============================================================\nüß† Model Configuration\n============================================================\nEncoder model      : microsoft/deberta-v3-base\nCounting layer     : count_lstm_v2\nToken pooling      : first\n============================================================\n\n\n\ndef gliner2(text: str) -&gt; dict:\n    schema = (model.create_schema()\n        .entities({\n            \"person\": \"A human individual\",\n            \"company\": \"An organization or company\",\n            \"location\": \"City or country\",\n            \"date\": \"Date\"\n        })\n        .classification(\"category\", [\"employment\", \"weather\", \"other\"])\n    )\n    return model.extract(text, schema)\n\n\n\nComparison\n\nclassical_nlp(docs_to_classify[0])\n\n{'entities': {'person': ['John Smith'],\n  'company': ['Acme Corp'],\n  'location': ['Berlin'],\n  'date': ['Jan 2021', 'late 2025']},\n 'category': 'employment'}\n\n\n\ngliner2(docs_to_classify[0])\n\n{'entities': {'person': ['John Smith'],\n  'company': ['Acme Corp'],\n  'location': ['Berlin'],\n  'date': ['late 2025', 'Jan 2021']},\n 'category': 'employment'}\n\n\n\nclassical_nlp(docs_to_classify[1])\n\n{'entities': {'person': [],\n  'company': [],\n  'location': ['Filadelphia'],\n  'date': []},\n 'category': 'weather'}\n\n\n\ngliner2(docs_to_classify[1])\n\n{'entities': {'person': [],\n  'company': [],\n  'location': ['Filadelphia'],\n  'date': []},\n 'category': 'weather'}\n\n\n\nclassical_nlp(docs_to_classify[2])\n\n{'entities': {'person': [],\n  'company': ['Anonumoys Inc.', 'NLP'],\n  'location': ['Warsaw', 'Poland'],\n  'date': ['February of 2026']},\n 'category': 'other'}\n\n\n\ngliner2(docs_to_classify[2])\n\n{'entities': {'person': [],\n  'company': ['Anonumoys Inc.'],\n  'location': ['Warsaw', 'Poland'],\n  'date': ['February of 2026']},\n 'category': 'other'}\n\n\n\n%%timeit\nclassical_nlp(docs_to_classify[2])\n\n727 ms ¬± 42.2 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\ngliner2(docs_to_classify[2])\n\n288 ms ¬± 29.5 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "posts/llms_for_dialogue_generation/index.html",
    "href": "posts/llms_for_dialogue_generation/index.html",
    "title": "Generate synthetic dialogues programatically for any topic",
    "section": "",
    "text": "Recently, I had a niche startup idea: helping elderly people navigate doctor appointments in the public health system in Poland. As a startupper, I should have found some ppl interested in such a service, and have interviews with them to learn more about their needs. But for a start I decided to use LLMs to generate me some synthetic conversations first to learn a bit more about the customer and his needs. This post walks through how I built this dialogue generation. This is all code driven, you‚Äôll need appropriate API keys to replicate it for your use case (TODO provide links to registration and key generation).\nThink of it as synthetic field research - you get the insights from number of conversations. This is cheating, and quality of those may vary depending on the area (if model has some information about things you want to discover, it may be useful for you). And because it‚Äôs LLM-generated, you can explore scenarios that are rare, uncomfortable to ask about, or haven‚Äôt happened yet.\n%pip install pydantic instructor openai dotenv\n\nRequirement already satisfied: pydantic in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (2.12.5)\nRequirement already satisfied: instructor in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (1.14.5)\nRequirement already satisfied: openai in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (2.16.0)\nRequirement already satisfied: dotenv in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (0.9.9)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from pydantic) (2.41.5)\nRequirement already satisfied: typing-extensions&gt;=4.14.1 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from pydantic) (4.15.0)\nRequirement already satisfied: typing-inspection&gt;=0.4.2 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from pydantic) (0.4.2)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.9.1 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (3.13.3)\nRequirement already satisfied: diskcache&gt;=5.6.3 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (5.6.3)\nRequirement already satisfied: docstring-parser&lt;1.0,&gt;=0.16 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (0.17.0)\nRequirement already satisfied: jinja2&lt;4.0.0,&gt;=3.1.4 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (3.1.6)\nRequirement already satisfied: jiter&lt;0.12,&gt;=0.6.1 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (0.11.1)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.32.3 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (2.32.5)\nRequirement already satisfied: rich&lt;15.0.0,&gt;=13.7.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (14.3.2)\nRequirement already satisfied: tenacity&lt;10.0.0,&gt;=8.2.3 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (9.1.2)\nRequirement already satisfied: typer&lt;1.0.0,&gt;=0.9.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from instructor) (0.21.1)\nRequirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from openai) (4.12.1)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx&lt;1,&gt;=0.23.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from openai) (0.28.1)\nRequirement already satisfied: sniffio in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm&gt;4 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from openai) (4.67.2)\nRequirement already satisfied: python-dotenv in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from dotenv) (1.2.1)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.5.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from aiohttp&lt;4.0.0,&gt;=3.9.1-&gt;instructor) (2.6.1)\nRequirement already satisfied: aiosignal&gt;=1.4.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from aiohttp&lt;4.0.0,&gt;=3.9.1-&gt;instructor) (1.4.0)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from aiohttp&lt;4.0.0,&gt;=3.9.1-&gt;instructor) (25.4.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from aiohttp&lt;4.0.0,&gt;=3.9.1-&gt;instructor) (1.8.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from aiohttp&lt;4.0.0,&gt;=3.9.1-&gt;instructor) (6.7.1)\nRequirement already satisfied: propcache&gt;=0.2.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from aiohttp&lt;4.0.0,&gt;=3.9.1-&gt;instructor) (0.4.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from aiohttp&lt;4.0.0,&gt;=3.9.1-&gt;instructor) (1.22.0)\nRequirement already satisfied: idna&gt;=2.8 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai) (3.11)\nRequirement already satisfied: certifi in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;openai) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;openai) (1.0.9)\nRequirement already satisfied: h11&gt;=0.16 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from httpcore==1.*-&gt;httpx&lt;1,&gt;=0.23.0-&gt;openai) (0.16.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from jinja2&lt;4.0.0,&gt;=3.1.4-&gt;instructor) (3.0.3)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.32.3-&gt;instructor) (3.4.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.32.3-&gt;instructor) (2.6.3)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from rich&lt;15.0.0,&gt;=13.7.0-&gt;instructor) (4.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from rich&lt;15.0.0,&gt;=13.7.0-&gt;instructor) (2.19.2)\nRequirement already satisfied: click&gt;=8.0.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from typer&lt;1.0.0,&gt;=0.9.0-&gt;instructor) (8.3.1)\nRequirement already satisfied: shellingham&gt;=1.3.0 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from typer&lt;1.0.0,&gt;=0.9.0-&gt;instructor) (1.5.4)\nRequirement already satisfied: mdurl~=0.1 in /home/misza222/Code/misza222.github.com/.venv/lib/python3.12/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&lt;15.0.0,&gt;=13.7.0-&gt;instructor) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\nfrom pydantic import BaseModel, Field\nfrom enum import Enum\nfrom typing import List, Optional\nimport re\nimport helper as h\nNR_OF_TURNS=15\nclass DialogueSpeaker(str, Enum):\n    CUSTOMER_SERVICE = 'CUSTOMER_SERVICE' # customer service assistant\n    CLIENT = 'CLIENT' # either person responsible for the eldery or disabled or payer\n\nclass DialogueTurn(BaseModel):\n    speaker: DialogueSpeaker = Field(..., description=\"Person speaking\")\n    content: str = Field(..., description=\"Exact text of what the person said\")\n\nclass DialogueScenario(BaseModel):\n    title: str = Field(..., description=\"Scenario for the conversation\")\n    dialogue: List[DialogueTurn] = Field(..., description=\"Conversation\")\nSCENARIO_SYSTEM_PROMPT = f\"\"\"\n# Customer Service Conversation Generator\n\nYou are generating a fictional but realistic in person customer discovery interviews in the spirit of lean startup methodology.\n\nThe problem that startupper found is lack of support for eldery and disabled patients within polish public health system.\nThere are often long queues and wait times, finding the right person to register with and doctors room for the visit may be \nchallenging in large clinics, or simply patient has difficulty remembering and understanding because of dementia and relatives \ndon't have time to assist him during the scheduled visit. There are also \n\nAsk open questions and validate assumptions.\n\nOutput Requirements: \n\n - Return ONLY valid JSON.  \n - No explanations, no commentary, no markdown, no prose outside the JSON.\n - The JSON must strictly conform to the DialogueScenario schema.\n - All fields must be present and correctly typed.\n - The dialogue array must contain at least {NR_OF_TURNS} turns representing a natural conversation.\n\"\"\"\nscenarios_user_messages = {\n    \"Elderly Patient Unable to Navigate NFZ Clinic Alone\": {\n        \"prompt\": \"Generate scenario for a customer discovery interview with an adult child describing how their elderly parent gets lost inside large NFZ clinics and cannot find the correct registration desk or doctor's room without assistance.\"\n    },\n    \"Relative Overwhelmed by NFZ Queue Logistics\": {\n        \"prompt\": \"Generate scenario for a customer discovery interview with a caretaker who explains the difficulty of managing long NFZ queues with an elderly patient who cannot stand for long periods or understand instructions and get's angry easily.\"\n    },\n    \"Dementia Patient Missing Appointments\": {\n        \"prompt\": \"Generate scenario for a customer discovery interview with a family member whose relative with dementia frequently forgets appointment times, required documents, or where to go inside the NFZ clinic.\"\n    },\n    \"Working Professional Unable to Accompany Parent\": {\n        \"prompt\": \"Generate scenario for a customer discovery interview with a busy professional who cannot take time off work to accompany their elderly parent to NFZ appointments, leading to missed or chaotic visits.\"\n    },\n}\ndef generate_dialogue_for_scenario(user_prompt, model=\"gpt-4o-mini\"):\n    messages = [\n        {'role': 'system', 'content': SCENARIO_SYSTEM_PROMPT},\n        {'role': 'user', 'content': user_prompt}\n    ]\n    \n    try:\n        result = h.call_api(messages, DialogueScenario, model=model)\n    except Exception as e:\n       print(f\"Full input: {e}\")\n    \n    return result\n# for scenario_name, scenario_dict in scenarios_user_messages.items():\n#     scenarios_user_messages[scenario_name]['results'] = generate_dialogue_for_scenario(scenario_dict['prompt'])\n#     with open(f\"data/scenarios_user_messages_{scenario_name}_results.json\", \"w\") as f:\n#         f.write(scenarios_user_messages[scenario_name]['results'].model_dump_json())\n\nfor scenario_name, scenario_dict in scenarios_user_messages.items():\n    with open(f\"data/scenarios_user_messages_{scenario_name}_results.json\") as f:\n        scenarios_user_messages[scenario_name]['results'] = DialogueScenario.model_validate_json(f.read())\nfor scenario_name, scenario_dict in scenarios_user_messages.items():\n    print(f\"# {scenario_name} {scenario_dict['results'].title}\")\n    for turn in scenario_dict['results'].dialogue:\n        print(f\"\\t{turn.speaker.value.upper()}: {turn.content}\")\n\n# Elderly Patient Unable to Navigate NFZ Clinic Alone Interview with an Adult Child of an Elderly Patient\n    CLIENT: Hi, I'm here to talk about my experience with my elderly parent visiting the local NFZ clinic.\n    CUSTOMER_SERVICE: Of course! Can you tell me about what challenges your parent faces when visiting the clinic?\n    CLIENT: Well, the first major issue is that the clinics are huge, and my parent often gets lost trying to find the registration desk.\n    CUSTOMER_SERVICE: That sounds frustrating. How does your parent usually navigate through the clinic?\n    CLIENT: They usually try to remember where things are, but with their memory issues, it becomes really difficult for them.\n    CUSTOMER_SERVICE: I can imagine. Have they ever asked someone for help when they feel lost?\n    CLIENT: Yes, sometimes they do, but it can be hard to find someone to ask, or they don‚Äôt want to bother others.\n    CUSTOMER_SERVICE: What about signs or maps in the clinic? Do they find those helpful?\n    CLIENT: Not really. The signs can be confusing and my parent often has a hard time understanding them.\n    CUSTOMER_SERVICE: I see. And what happens when they finally get to the registration desk?\n    CLIENT: The staff there are usually quite busy, and my parent might forget their appointment details or feel rushed.\n    CUSTOMER_SERVICE: That sounds stressful. Do you ever accompany them to the appointments?\n    CLIENT: I try to, but with my work schedule, it's hard to be there every time.\n    CUSTOMER_SERVICE: What do you think would help make the process easier for your parent?\n    CLIENT: Having someone available to guide them through the clinic would be ideal, or even a helper service for elderly patients.\n    CUSTOMER_SERVICE: That sounds like a great idea. Is there anything else you think could improve their experience?\n    CLIENT: More staff dedicated to helping patients navigate the clinic would make a huge difference.\n# Relative Overwhelmed by NFZ Queue Logistics Customer Discovery Interview with a Caretaker of an Elderly Patient\n    CLIENT: Hello, I'm here to talk about the experiences I've had taking care of my elderly father.\n    CUSTOMER_SERVICE: Thank you for coming in today. Can you tell me more about the challenges you face when managing visits to the healthcare system?\n    CLIENT: One of the biggest issues is the long queues at the NFZ clinics. My father often can't stand for long periods.\n    CUSTOMER_SERVICE: I see, that must be very difficult for both of you. How does he react during these long waits?\n    CLIENT: He gets frustrated easily, especially when he doesn't understand what‚Äôs happening or what he needs to do next.\n    CUSTOMER_SERVICE: That sounds challenging. How do you manage those situations when he becomes angry?\n    CLIENT: I try to remain calm and reassure him, but it can be tough. Sometimes he just doesn't want to cooperate anymore.\n    CUSTOMER_SERVICE: Have you found any strategies or support that help during these visits?\n    CLIENT: Not really. I often feel overwhelmed and wish there were more resources available to assist elderly patients like him.\n    CUSTOMER_SERVICE: Is it difficult for you to navigate the system as well, in finding the right department or person?\n    CLIENT: Yes, very much so. Sometimes we go to the wrong room and then have to start all over again.\n    CUSTOMER_SERVICE: That sounds frustrating. How does this impact his healthcare access?\n    CLIENT: It makes it harder for him to get the help he needs regularly. Sometimes we just give up altogether.\n    CUSTOMER_SERVICE: What improvements do you think could be made to help you and others in similar situations?\n    CLIENT: Having someone to assist with navigation would be helpful, maybe a dedicated caretaker for elderly patients.\n    CUSTOMER_SERVICE: That‚Äôs a great suggestion. Would you be open to exploring solutions that could provide that support?\n    CLIENT: Absolutely. Anything that makes it easier for us would be welcome.\n# Dementia Patient Missing Appointments Customer Discovery Interview with a Family Member of a Patient with Dementia\n    CLIENT: Hi, I'm really struggling to manage my relative's medical appointments. They often forget when and where to go.\n    CUSTOMER_SERVICE: That sounds very challenging. Could you tell me more about the typical issues you face with their appointments?\n    CLIENT: Sure. Sometimes they forget the time of the appointment completely, and often they don't know what documents they need to bring.\n    CUSTOMER_SERVICE: I see. How do you currently remind them of their appointments?\n    CLIENT: I try to call them or send them messages, but they often can't remember the information or get confused.\n    CUSTOMER_SERVICE: That must be frustrating. Do they recognize the clinic when you arrive?\n    CLIENT: Not always. Sometimes they forget the way or the name of the clinic, so we spend a lot of time just trying to find the right place.\n    CUSTOMER_SERVICE: How do you handle it when they can't remember where to go?\n    CLIENT: I usually have to go with them, but I have a busy schedule, and my siblings are also busy.\n    CUSTOMER_SERVICE: Have you ever considered using any kind of reminders or technology to assist with this?\n    CLIENT: I've thought about it, but I‚Äôm not sure which tools would work best for someone with dementia.\n    CUSTOMER_SERVICE: What kind of support do you think would help you the most in this situation?\n    CLIENT: Maybe having someone who could assist us at the clinic would be helpful. Just someone who knows what to do.\n    CUSTOMER_SERVICE: That sounds like a great idea. Would you prefer this assistance to be available on-site at the clinic or remotely through a phone app or service?\n    CLIENT: On-site would be best since they need physical help navigating.\n    CUSTOMER_SERVICE: Thank you for sharing this with me. Is there anything else that you think could improve the experience for both you and your relative?\n    CLIENT: Just having clearer communication from the clinic about what they need and when would really help as well.\n# Working Professional Unable to Accompany Parent Customer Discovery Interview with a Busy Professional\n    CLIENT: Hi, thank you for meeting with me today. I really appreciate your time.\n    CUSTOMER_SERVICE: Of course! I'm glad to help. Can you tell me a bit about your situation with your elderly parent?\n    CLIENT: Well, my father is quite frail and needs regular check-ups at the clinics, but my work schedule is so demanding.\n    CUSTOMER_SERVICE: That sounds challenging. How often does he have appointments?\n    CLIENT: Usually once a month, but sometimes it's more frequent if he feels unwell.\n    CUSTOMER_SERVICE: Have you had any issues with getting him to those appointments?\n    CLIENT: Yes, there are times when I can‚Äôt take time off work, and he ends up either missing the appointment or not having someone to support him.\n    CUSTOMER_SERVICE: I see. How does he manage the visit alone when you can't accompany him?\n    CLIENT: He gets confused sometimes, especially in large clinics where it's hard to find the right office.\n    CUSTOMER_SERVICE: That must be stressful for both of you. Have you looked into any ways he could receive support?\n    CLIENT: Not really. I don‚Äôt know where to start, and my relatives are also busy with their jobs.\n    CUSTOMER_SERVICE: If there were a service that could help him navigate those appointments, would you be interested?\n    CLIENT: Definitely! It would need to be reliable though, as I can't afford to have him get lost or miss a visit.\n    CUSTOMER_SERVICE: Are there specific features you think would be necessary for such a service?\n    CLIENT: Yes, maybe someone to assist him on-site, and a way for me to receive updates or manage appointments remotely.\n    CUSTOMER_SERVICE: That sounds practical. How do you think your father would feel about having someone assist him?\n    CLIENT: I think he would feel much more comfortable and less anxious knowing someone is there to help."
  },
  {
    "objectID": "posts/llms_for_dialogue_generation/index.html#tts",
    "href": "posts/llms_for_dialogue_generation/index.html#tts",
    "title": "Generate synthetic dialogues programatically for any topic",
    "section": "TTS",
    "text": "TTS\nYou can stop there but, it would be nice to actually hear it. For that it need to be augmented with TTS tags:\n\nfrom elevenlabs import ElevenLabs\nfrom dotenv import load_dotenv\nfrom IPython.display import Audio\nimport os\n\n\nclient = ElevenLabs(\n    api_key = os.getenv(\"ELEVENLABS_API_KEY\")\n)\n\n\nVOICE_ENHACEMENT_FOR_11LABS_SYSTEM_PROPMT = \"\"\"\nYour task:\n\n# Preserve the original meaning and intent of each utterance.\n\n## Add ElevenLabs tags, such as:\n\n- &lt;voice emotion=\"...\"&gt; (e.g., empathetic, calm, encouraging, reflective, tense, relieved)\n- &lt;break time=\"...ms\"&gt;\n- &lt;prosody rate=\"...\" pitch=\"...\"&gt;\n- &lt;emphasis&gt;\n\n## Enhance the dramatic flow of the conversation by:\n\n- highlighting the client‚Äôs emotions,\n- adding subtle pauses,\n- marking moments of tension, relief, or reflection.\n\n## Do not change the meaning, but you may:\n\n- add natural pauses,\n- add effects using tags such as\n[laughs], [laughs harder], [starts laughing], [wheezing], [whispers], [sighs], [exhales], [sarcastic], [curious], [excited], [crying], [snorts], [mischievously],\n- emphasize emotional transitions.\n\n## Each CUSTOMER_SERVICE and CLIENT utterance must have its own block with tags.\n\n## Do not add narration or commentary ‚Äî only the dialogue.\n\"\"\"\n\nVOICE_ENHACEMENT_FOR_11LABS_USER_PROPMT = \"Find below dialogues to enhance:\"\n\n\ndef enhance_dialog_for_elevenlabs(dialog):\n        user_prompt = VOICE_ENHACEMENT_FOR_11LABS_USER_PROPMT\n        for turn in dialog:\n            user_prompt += f\"\\t{turn.speaker.value.upper()}: {turn.content}\\n\"\n        messages = [\n            {'role': 'system', 'content': VOICE_ENHACEMENT_FOR_11LABS_SYSTEM_PROPMT},\n            {'role': 'user', 'content': user_prompt}\n        ]\n        \n        return h.call_api(messages, DialogueScenario)\n\nfor scenario_name, scenario_dict in scenarios_user_messages.items():\n    filename = f\"data/scenarios_user_messages_{scenario_name}_enhanced_results.json\"\n    if not os.path.isfile(filename):\n        print(f\"Processing scenario {scenario_name}\")\n        \n        result = enhance_dialog_for_elevenlabs(scenario_dict['results'].dialogue)\n        scenario_dict['results_for_voice'] = result\n        \n        with open(filename, \"w\") as f:\n            f.write(scenarios_user_messages[scenario_name]['results_for_voice'].model_dump_json())\n    else:\n        with open(filename) as f:\n            scenarios_user_messages[scenario_name]['results_for_voice'] = DialogueScenario.model_validate_json(f.read())\n\n‚Ä¶ and finally generate voice with elevenlabs (but you could use other TTS model like VibeVoice from Microsoft)\n\nimport io\nfrom IPython.display import Audio\nimport json\nimport hashlib\n\ndef hash_dict(d: dict) -&gt; str:\n    # Convert dict to a canonical JSON string\n    encoded = json.dumps(d, sort_keys=True).encode(\"utf-8\")\n    return hashlib.sha256(encoded).hexdigest()\n\ndef speak(text: str, voice_id: str, voice_settings: dict) -&gt; bytes:\n    audio_stream = client.text_to_speech.convert(\n        voice_id=voice_id,\n        voice_settings=voice_settings,\n        output_format=\"mp3_44100_128\",\n        text=text,\n        model_id=\"eleven_flash_v2_5\"\n    )\n    return b\"\".join(audio_stream)\n\n\ndef speak_dialog(dialog: list[dict], voice_map: dict) -&gt; Audio:\n    \"\"\"\n    dialog = [\n        {\"speaker\": \"CUSTOMER_SERVICE\", \"text\": \"...\"},\n        {\"speaker\": \"CLIENT\", \"text\": \"...\"},\n    ]\n\n    voice_map = {\n        \"CUSTOMER_SERVICE\": {\"voice_id\": \"voice_id_1\", voice_settings = {},}\n        \"CLIENT\": {\"voice_id\": \"voice_id_1\", voice_settings = {},}\n    }\n    \"\"\"\n\n    dialog_hash = hash_dict(dialog)\n    dialog_filename = f'data/{dialog_hash}.raw'\n    \n    if not os.path.isfile(dialog_filename):\n        combined = b\"\"\n        \n        for turn in dialog:\n            speaker = turn[\"speaker\"]\n            text = turn[\"text\"]\n            voice_id = voice_map[speaker]['voice_id']\n            voice_settings = voice_map[speaker]['voice_settings']\n    \n            audio_bytes = speak(text, voice_id, voice_settings)\n    \n            # Dodaj kr√≥tkƒÖ pauzƒô miƒôdzy wypowiedziami (silence MP3)\n            silence = b\"\\x00\" * 4000  # ~2kB ciszy, dzia≈Ça w wiƒôkszo≈õci player√≥w\n    \n            combined += audio_bytes + silence\n            print(\".\", end=\"\")\n            \n        with open(dialog_filename, \"wb\") as f:\n            f.write(combined)\n    else:\n        print(\"!!!CACHED!!!\", end=\"\")\n    with open(dialog_filename, \"rb\") as f:\n        combined = f.read()\n\n    return Audio(combined, autoplay=True)\n\n\ndef generate_audio_for(scenario):\n    dialog = []\n    for turn in scenarios_user_messages[scenario]['results_for_voice'].dialogue:\n        dialog.append({'speaker': turn.speaker.value.upper(), 'text': turn.content})\n    \n    voice_map = {\n        \"CUSTOMER_SERVICE\": {\n            \"voice_id\": \"N0GCuK2B0qwWozQNTS8F\", \n            'voice_settings': {\n                \"stability\": 0.5, \n                \"similarity_boost\": 0.75,\n                \"style\": 0.35\n            }\n        },\n        \"CLIENT\": {\n            'voice_id': \"TxGEqnHWrfWFTfGW9XjX\", \n            'voice_settings': {\n                \"stability\": 0.3,\n                \"similarity_boost\": 0.5,\n                \"style\": 0.8,\n                \"use_speaker_boost\": True\n            }\n        },\n    }\n    # print(voice_map)\n    return speak_dialog(dialog, voice_map)\n\n\nfor dialog_title in list(scenarios_user_messages.keys()):\n    print(dialog_title, end=\"\")\n    generate_audio_for(dialog_title)\n    print(\"\")\n\nElderly Patient Unable to Navigate NFZ Clinic Alone!!!CACHED!!!\nRelative Overwhelmed by NFZ Queue Logistics!!!CACHED!!!\nDementia Patient Missing Appointments!!!CACHED!!!\nWorking Professional Unable to Accompany Parent!!!CACHED!!!\n\n\n\ngenerate_audio_for(list(scenarios_user_messages.keys())[0])\n\n!!!CACHED!!!\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nAs you can see, this is relatively simple way to dive into synthetic conversations, and this is a good start to learning more about new domain that you don‚Äôt have experience with."
  }
]