[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Technical notes",
    "section": "",
    "text": "Best way to do traditional NLP?\n\n\nGLiNER2 and it‚Äôs example usage\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\nFeb 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nNeRF basics\n\n\n\n\n\n\nnerf\n\n\n\ncompanion post for DSS 2023 conference\n\n\n\n\n\nNov 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTraining vision models on synthetic images\n\n\n\n\n\n\npaper review\n\nvision\n\n\n\nNot SOTA performance, but quite good\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhy use F.cross_entropy?\n\n\n\n\n\n\ntraining\n\n\n\n‚Ä¶ instead of computing it yourself?\n\n\n\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOptuna is hot on Kaggle\n\n\n\n\n\n\ntraining\n\n\n\nHow to use it with code examples\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to read papers?\n\n\n\n\n\n\npaper review\n\n\n\nGreat advice from Andrew Ng\n\n\n\n\n\nDec 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMath resources\n\n\n\n\n\n\nmath\n\n\n\nAll you need to within weeks understand Deep Learning and be reasonably comfortable with papers.\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "example_posts/post-with-code/index.html",
    "href": "example_posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/math_resources/index.html",
    "href": "posts/math_resources/index.html",
    "title": "Math resources",
    "section": "",
    "text": "My Uni days are long gone, I was reasonably good at math, but over the years of not using it, I almost feel like the knowledge was never there.\nHere is my journey through the process of remembering some math that I need to feel more comfortable with for the basics of deep learning and to be able to digest papers in the broad area of deep learning research.\nAdvice to my former self: First read some papers, struggle through them, let the frustration build up so you have a motivation to learn + you will also build an intuition of what tools you may actually need!\nAs for the resouces, I started with Deep Learninig book few years ago, but got discouraged by the theory and I didn‚Äôt have enough practice to know that it will be useful some day.\nRecently I found out that there is a growing comunity of people around that book led by Sanyam Buthani. Apart from the community and help + motivation to learn that comes with it, there are number of resources for self study, such as notebooks with all the concepts translated into code, which makes it so much more practical.\nStaying around the same book, there are great lectures going through each chapter of that book.\nIf you want to learn more about torch.autograd, frameworks that do automatic differentiation in general and understand calculus that is the engine of deep learning machine there is an excelent The Matrix Calculus You Need For Deep Learning.\nAnother interesting resources are Mathematics for Machine Learning and short lectures covering pretty much all you need for the topic in a very accessible, visual and short form.\nLast but not least, Andrew Ng‚Äôs advice on how to read a paper which I also sumarize here!\nAll these resources are freely available online."
  },
  {
    "objectID": "posts/how_to_read_papers/index.html",
    "href": "posts/how_to_read_papers/index.html",
    "title": "How to read papers?",
    "section": "",
    "text": "Andrew Ng gave an career advice lecture at Stanford in 2019 where he also mentioned how to read academic papers.\nRead it taking multiple passes through the paper:\n\nTitle + Abstract + Figures\nIntro + Conclusions + Figures + Skim rest\nRead text but skip math\nRead all of it but skip what doesn‚Äôt make sense\n\nwhich boils down to:\nGo from efficient and high information content first and dig into the harder bits gradually\nSome of the questions to keep in mind during the process (and to decide whether to go to the next step):\n\nWhat did authors try to acomplish?\nWhat are the key elements of the approach?\nWhat can you use yourself?\nWhat other references do you want to follow?\n\nIt is worth watching, as it also sumarizes the process of creating and reviewing the paper to give rationale for that process."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Husband, father, Christian, programmer, fascinated with NNs and most recently with Computer Vision, specifically NeRFs.\nI took a sabbatical to explore machine/deep learning but I am open to offers that will lead me into direction of greater enlightement in a field of Deep Learning, CG and applications in Med/Pharma. Can be fully Remote or in Warsaw/Poland.\nI write this blog mostly for:\n\nmyself to better remember what I learn (and I have tons of blog posts that I never publish as no time to polish it for other to learn from)\nothers to learn from - I often post about stuff that I learn with others (such as HF RL course topics) - it is easier to convince yourself to write good quality content with that objective in mind :)\nrectuiters/employers - so they can discover what I learn and what I enjoy doing\n\nIf you are on a learning path like me, I can‚Äôtrecommend enough writing a blog.\nIf you are interested in my careed so far see Curriculum Vitae\nMost images dreamt by Stable Diffusion and DALL¬∑E 3, hosted on github pages, rendered with Quarto."
  },
  {
    "objectID": "posts/optuna_notes/index.html",
    "href": "posts/optuna_notes/index.html",
    "title": "Optuna is hot on Kaggle",
    "section": "",
    "text": "While doing HF RL course I bumped into Optuna, then I‚Äôve noticed that ppl do it on Kaggle a lot - this is like a badge of honor for a package if it is being used on a top ML competition site ergo it is worth learning!\nSome vocab to get started:\n\nobjective(trial) function - function to optimize\ntrial - a single test, also an object passed to the objective function\nstudy - a set of trial at the end of which you get a suggestion of parameters to use\nparameter - parameter to optimize\nsetting initial values for parameters to optimize:\n\noptuna.trial.Trial.suggest_categeorical(‚Äòname‚Äô, [‚Äòlist‚Äô])\noptuna.trial.Trial.suggest_int(‚Äòname‚Äô, min, max)\noptuna.trial.Trial.suggest_float(‚Äòname‚Äô, min, max)\n\n\nHere is a quick summary of how to use it:\n\n\nCode\n%pip install optuna\n\n\n\nimport optuna\n# to supress unnecessary output as it prints quite a lot by default\noptuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# Task: with 100 trials find a minimum for a function (x-10)**2\n\n# objective function to minimize\ndef objective(trial):\n    # this is just returning float and internally in the trial optuna \n    # keeps track of all the values used\n    x = trial.suggest_float(\"x\", -100, 100) \n    return (x - 10)**2\n\n# create optimization object that will keep track of the whole process\nstudy = optuna.create_study() \n\n# and run optimization with 100 runs\nstudy.optimize(objective, n_trials=100)\n\n# get the optimized values\nstudy.best_params['x'] # we are pretty close\n\n10.108796067394927\n\n\n\n# however it won't do magic if you don't give it enough \"space\"\n# here if you give it just 10 trials, it will usually miss quite \n# substantially\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -100, 100)\n    return (x - 10)**2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\n\nstudy.best_params['x'] # ... it is usually not so good\n\n-10.114148942248292\n\n\nhow optuna works internally is quite simple but ingenious: each call to trial.suggest_*() function already returns a python variable, so you can use it in your code straight away:\n\nstudy = optuna.create_study()\ndef objective(trial):\n    # trial.suggest_int returns integer - all the magic of storing\n    # what value was used in a specific trial is recorded in trial object\n    i = trial.suggest_int('x', 0, 100, step=10)\n    print(f\"next {i=}\")\n    return i\n\nstudy.optimize(objective, n_trials=10)\nstudy.best_params['x']\n\nnext i=0\nnext i=0\nnext i=90\nnext i=60\nnext i=70\nnext i=10\nnext i=10\nnext i=10\nnext i=60\nnext i=90\n\n\n0\n\n\nIt is interesting at first how the numbers are drawn from the space - this is all quasi random and duplicates are possible. Especially if we have very limited space of available unique values as in here. This is not an implementation bug - here we deal with a single variable, but if we have multiple ones, it quite makes sense to try simillar values if we variate other parameters at the same time. This is default, but you can choose different strategires for drawing values.\n\n\n\n\n\n# however with 10 trials...\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -100, 100)\n    return (x - 10)**2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"After 10 trials we got {study.best_params['x']=:.2f}\") # ... it is usually not so good\n\n# but training for another 10 iterations does the trick\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"... but 10 more runs get us closer {study.best_params['x']=:.2f}\") # ok, now it is better :)\n\nAfter 10 trials we got study.best_params['x']=1.35\n... but 10 more runs get us closer study.best_params['x']=9.38\n\n\n\n\n\nOr just run hiperparameter searches when your colab disconnects\nOptuna allows for distrubuted trials\n\n#straight from optuna docs @ https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10)\n    return (x - 2) ** 2\n\n\nif __name__ == \"__main__\":\n    study = optuna.load_study(\n        study_name=\"distributed-example\", storage=\"mysql://root@localhost/example\"\n    )\n    study.optimize(objective, n_trials=100)\n\n\n\n\nIt was a bit tricky to understand for me how it works, as it is usually hidden in handlers to specific libraries. But here is a clear example that doesn‚Äôt hide anything from you:\n\noptuna.logging.set_verbosity(optuna.logging.INFO)\nimport random\n\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10) # you draw next value\n\n    # and this is an inner loop simulating inner loop in the \n    # optimization functions, like going through batches in\n    # NN training\n    for i in reversed(range(10)): \n        # here we just make up a number simulating intermediate result\n        # that is sent to optuna to validate if it is worth continuing\n        made_up_intermediate_value = random.randint(1, 10)\n        # it is reported to optuna\n        trial.report(made_up_intermediate_value, i)\n\n        # Handle pruning based on the intermediate value.\n        if trial.should_prune(): # Optuna suggests to prune?\n            print(f'''\\\nPruning trial {trial.number} with value {made_up_intermediate_value=}\\n\\\nbecause it is already less optimal than previously recorded best value''', flush=True, end='')\n            # if yes we throw exception that is handled by `optimize` method\n            # in optuna\n            raise optuna.TrialPruned() \n\n\n\n    return (x - 2) ** 2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\nstudy.best_params['x']\n\n\n[I 2023-01-13 13:49:05,151] A new study created in memory with name: no-name-826daad7-a560-4139-8012-384a08aecae9\n\n[I 2023-01-13 13:49:05,154] Trial 0 finished with value: 27.811950820406743 and parameters: {'x': 7.2737037099562905}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,157] Trial 1 finished with value: 81.56034995467249 and parameters: {'x': -7.0310768989458}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,162] Trial 2 finished with value: 1.8078821300442816 and parameters: {'x': 3.3445750741569924}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,165] Trial 3 finished with value: 51.599347421072665 and parameters: {'x': 9.183268575034116}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,168] Trial 4 finished with value: 87.26649837508509 and parameters: {'x': -7.341653942160622}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 5 with value made_up_intermediate_value=10\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,170] Trial 5 pruned. \n\n[I 2023-01-13 13:49:05,181] Trial 6 finished with value: 26.47065704510398 and parameters: {'x': -3.1449642413824392}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,185] Trial 7 finished with value: 29.163269432320956 and parameters: {'x': 7.400302716729957}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 8 with value made_up_intermediate_value=6\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,187] Trial 8 pruned. \n\n[I 2023-01-13 13:49:05,194] Trial 9 finished with value: 38.838470030340275 and parameters: {'x': 8.232051831486984}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\n3.3445750741569924"
  },
  {
    "objectID": "posts/optuna_notes/index.html#useful-tricks",
    "href": "posts/optuna_notes/index.html#useful-tricks",
    "title": "Optuna is hot on Kaggle",
    "section": "",
    "text": "# however with 10 trials...\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -100, 100)\n    return (x - 10)**2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"After 10 trials we got {study.best_params['x']=:.2f}\") # ... it is usually not so good\n\n# but training for another 10 iterations does the trick\nstudy.optimize(objective, n_trials=10)\n\nprint(f\"... but 10 more runs get us closer {study.best_params['x']=:.2f}\") # ok, now it is better :)\n\nAfter 10 trials we got study.best_params['x']=1.35\n... but 10 more runs get us closer study.best_params['x']=9.38\n\n\n\n\n\nOr just run hiperparameter searches when your colab disconnects\nOptuna allows for distrubuted trials\n\n#straight from optuna docs @ https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10)\n    return (x - 2) ** 2\n\n\nif __name__ == \"__main__\":\n    study = optuna.load_study(\n        study_name=\"distributed-example\", storage=\"mysql://root@localhost/example\"\n    )\n    study.optimize(objective, n_trials=100)\n\n\n\n\nIt was a bit tricky to understand for me how it works, as it is usually hidden in handlers to specific libraries. But here is a clear example that doesn‚Äôt hide anything from you:\n\noptuna.logging.set_verbosity(optuna.logging.INFO)\nimport random\n\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10) # you draw next value\n\n    # and this is an inner loop simulating inner loop in the \n    # optimization functions, like going through batches in\n    # NN training\n    for i in reversed(range(10)): \n        # here we just make up a number simulating intermediate result\n        # that is sent to optuna to validate if it is worth continuing\n        made_up_intermediate_value = random.randint(1, 10)\n        # it is reported to optuna\n        trial.report(made_up_intermediate_value, i)\n\n        # Handle pruning based on the intermediate value.\n        if trial.should_prune(): # Optuna suggests to prune?\n            print(f'''\\\nPruning trial {trial.number} with value {made_up_intermediate_value=}\\n\\\nbecause it is already less optimal than previously recorded best value''', flush=True, end='')\n            # if yes we throw exception that is handled by `optimize` method\n            # in optuna\n            raise optuna.TrialPruned() \n\n\n\n    return (x - 2) ** 2\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)\nstudy.best_params['x']\n\n\n[I 2023-01-13 13:49:05,151] A new study created in memory with name: no-name-826daad7-a560-4139-8012-384a08aecae9\n\n[I 2023-01-13 13:49:05,154] Trial 0 finished with value: 27.811950820406743 and parameters: {'x': 7.2737037099562905}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,157] Trial 1 finished with value: 81.56034995467249 and parameters: {'x': -7.0310768989458}. Best is trial 0 with value: 27.811950820406743.\n\n[I 2023-01-13 13:49:05,162] Trial 2 finished with value: 1.8078821300442816 and parameters: {'x': 3.3445750741569924}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,165] Trial 3 finished with value: 51.599347421072665 and parameters: {'x': 9.183268575034116}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,168] Trial 4 finished with value: 87.26649837508509 and parameters: {'x': -7.341653942160622}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 5 with value made_up_intermediate_value=10\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,170] Trial 5 pruned. \n\n[I 2023-01-13 13:49:05,181] Trial 6 finished with value: 26.47065704510398 and parameters: {'x': -3.1449642413824392}. Best is trial 2 with value: 1.8078821300442816.\n\n[I 2023-01-13 13:49:05,185] Trial 7 finished with value: 29.163269432320956 and parameters: {'x': 7.400302716729957}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\nPruning trial 8 with value made_up_intermediate_value=6\nbecause it is already less optimal than previously recorded best value\n\n\n\n[I 2023-01-13 13:49:05,187] Trial 8 pruned. \n\n[I 2023-01-13 13:49:05,194] Trial 9 finished with value: 38.838470030340275 and parameters: {'x': 8.232051831486984}. Best is trial 2 with value: 1.8078821300442816.\n\n\n\n\n3.3445750741569924"
  },
  {
    "objectID": "posts/f_cross_entropy/index.html",
    "href": "posts/f_cross_entropy/index.html",
    "title": "Why use F.cross_entropy?",
    "section": "",
    "text": "apart from less code of course! Another reason: it is safer!\nI like to have more controll over what and how I am doing things, instead of using black boxes. But be warned that you can get burned when computing negative log likelihood yourself (the same is tru for softmaxes for example).\nsee this example:\n\nimport torch\n\nlogits = torch.tensor([-100, -5, 2, 100])\nlogits = logits.exp()\nprobs = logits / logits.sum()\nprobs, probs.sum()\n\n(tensor([0., 0., 0., nan]), tensor(nan))\n\n\nmakes sense, right? exp(100) is VERY large, so if your network misbehaves and produces extreme activations, you have a problem, but‚Ä¶\n\nimport torch\n\nlogits = torch.tensor([-100, -5, 2, 100])\n\n# here we deduct max value from the logits, so everyting is in (-‚àû, 0)\n#----------------------\nlogits -= logits.max() \n#----------------------\n\nlogits = logits.exp()\nprobs = logits / logits.sum()\nprobs, probs.sum()\n\n(tensor([0.0000e+00, 0.0000e+00, 2.7465e-43, 1.0000e+00]), tensor(1.))\n\n\nis working nicely, and that‚Äôs what F.cross_entropy does internally. Of course, you can always add that normalization to safeguard against such cases (or add batchnorm layers to your architecture if you don‚Äôt wan‚Äôt to bother about such cases at the cost of a little more complexity and state in your model).\nPlus of course I am sure there are also more good computational efficiency reasons to use torch‚Äôes built-in method do that."
  },
  {
    "objectID": "posts/nerf_basics/index.html",
    "href": "posts/nerf_basics/index.html",
    "title": "NeRF basics",
    "section": "",
    "text": "This is a companion blog post for my presentation given at Data Science Summit 2023 in Warsaw.\nThe presentation itself is on google docs  and recording of the presentation will be published later.\n\n\nRight now I have not much to add to the presentation. But make sure to check out extensive speaker notes there.\nBelow is the dozer that I used as a training data for nerf.studio that is rendered here with poly.cam and lumalabs.ai"
  },
  {
    "objectID": "posts/synthetic_vision_training/index.html",
    "href": "posts/synthetic_vision_training/index.html",
    "title": "Training vision models on synthetic images",
    "section": "",
    "text": "What if we‚Äôll be able to train vision NN model with 0, nil, nada real world examples? But we have to train on something, right? Surely training on a pure noise won‚Äôt give us anything useful? See CNN-Rand or StyleGAN-Random (both initialized randomly, no training whatsoever) below and you will be surprised! Moreover if we construct synthetic images that are closer to the real world images, can we train on it with some positive outcomes? But then how well can we train? Paper presented below tried to answer that question.\nUnless tagged otherwise, ideas in this blog post come from the Learning to See by Looking at Noise paper, recorded presentation by Antonio Torralba who is one of the authors and presentation by Rosanne Liu during Deep Learning: Classics and Trends.\n\nRationale for this exercise\nModels are more and more reliant on data. CLIP need 400 000 000 images to be trained well for example. What if we could build a synthetic dataset to train? Why:\n\nyou don‚Äôt have access to data\ncheaper to maintain the data\nmaybe generating a good synthetic dataset can be better than real data (no human bias for example)\n\n\n\nTask\nThe training objective is classification of ImageNet-100 images.\n\n\nTraining procedure\nIt is done in 2 stages:\n\nThe ‚Äúbase‚Äù network is trained using unsupervised contrastive learning (simplifying it is done by identifying if images are the same or come from the same source image with transformations applied; details of the specific approach used in the paper)\nFinal layer (but could be layers I think) are trained briefly on actual data to create a head of the model (to me it was not clear reading the paper but see this training script by authors)\n\nTechnicalities derived from the code:\n\nunsupervised part (1):\n\nmodel parameters: TBD\nepochs: 200\ntime: TBD (most expensive part)\n\nsupervised part (2):\n\nmodel parameters: TBD\nepochs: 100\ntime: TBD (but as it is training just single fc layer, this will be cheap)\n\n\n\n\nDatasets\nSee this image of datasets used in this experiment which are referred blow on the benchmark graph.\n\n\n\nResults\n\nBlack bars are different baselines and coloured bars represent various sythetic datasets. About 20% is a difference between the best model trained on actual images and best model pre-trained on synthetic images. The surprising bit is, that randomly initialized networks can give much better than chance results (and random choice is 1% as we have 100 classes): CNN - Rand and StyleGAN - Random. The explanation was, that last fully connected layer will give some performance boost, but Antonio in his video also mentions that even some of those randomly initialized ‚Äúfeatures must be useful to some degree‚Äù. So for example in CNN some of the filters extract information that is then used by linear layer to reason upon.\n\n\nMy Conclusions\nCan this approach democratize access to data, as currently data collection and maintenance is being more and more concentrated? It looks like it, but for now it comes at the cost of performance. There was also a lot of laughter during the Antonios presentation about Stable Diffusion, so I have to add that: what would be the result if data was generated by SD model?"
  },
  {
    "objectID": "example_posts/welcome/index.html",
    "href": "example_posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/gliner2/index.html",
    "href": "posts/gliner2/index.html",
    "title": "Best way to do traditional NLP?",
    "section": "",
    "text": "We use LLMs for everything nowadays, from adding numbers, to finding a meaning of life. But no, it is not another post about Large Language Models as they are often not optimal (accuracy, reliability, environment, cost)-wise? But it is about good old NLP and about what it seems like use-cases from previous century, but still very usefull and with positive ROI.\nBeing consumed by LLMs, agents and stuff, I accidentally bumped into transformers based information extraction system and as an exercise tried it to replace some of my nlp code for NER and classification to use GLiNER2:\n\nExtract entities, classify text, parse structured data, and extract relations‚Äîall in one efficient model.\n\nTL;DR: the result was shorter code, faster inference, and a much cleaner pipeline.\nSo what I was trying to do:\n\nextract some names and concepts,\nclassify documents with custom labels\n\n‚Ä¶ but just look at the code and benchmarks\n\nTraditional NER and zero shot classification\n\n# !pip install spacy transformers numpy torch\n\n\n# !spacy download en_core_web_lg\n\n\n# Load NLP\nimport spacy\nfrom transformers import pipeline\nimport re\nimport numpy as np\nnlp = spacy.load(\"en_core_web_lg\")\nclassifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\"\n)\n\n\n\n\n\ndocs_to_classify = [\n    \"John Smith joined Acme Corp in Jan 2021 and worked there until late 2025 as a senior engineer based in Berlin.\",\n    \"It is always sunny in Filadelphia\",\n    \"For us, Anonumoys Inc. based in Warsaw, Poland, traditional NLP is more predictible then LLMs, at least in February of 2026\",\n]\n\n\ndef classical_nlp(text:str) -&gt; dict:\n    # NER\n    doc = nlp(text)\n\n    entities = {\n        \"PERSON\": [],\n        \"ORG\": [],\n        \"GPE\": [],\n        \"DATE\": []\n    }\n    \n    for ent in doc.ents:\n        if ent.label_ in entities:\n            entities[ent.label_].append(ent.text)\n    \n    # Classification\n    labels = [\"employment\", \"weather\", \"other\"]\n    clf = classifier(text, candidate_labels=labels)\n    \n    doc_type = clf[\"labels\"][np.argmax(clf[\"scores\"])]\n\n    # glue it together\n    return {\n        \"entities\": {\n        \"person\": entities[\"PERSON\"],\n        \"company\": entities[\"ORG\"],\n        \"location\": entities[\"GPE\"],\n        \"date\": entities[\"DATE\"],\n        },\n        \"category\": doc_type\n    }\n\n\n\n‚Ä¶ same with GLiNER2\n\n# !pip install gliner2\n\n\n# Load GLiNER\nfrom gliner2 import GLiNER2\nmodel = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")\n\nYou are using a model of type extractor to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n\n\n============================================================\nüß† Model Configuration\n============================================================\nEncoder model      : microsoft/deberta-v3-base\nCounting layer     : count_lstm_v2\nToken pooling      : first\n============================================================\n\n\n\ndef gliner2(text: str) -&gt; dict:\n    schema = (model.create_schema()\n        .entities({\n            \"person\": \"A human individual\",\n            \"company\": \"An organization or company\",\n            \"location\": \"City or country\",\n            \"date\": \"Date\"\n        })\n        .classification(\"category\", [\"employment\", \"weather\", \"other\"])\n    )\n    return model.extract(text, schema)\n\n\n\nComparison\n\nclassical_nlp(docs_to_classify[0])\n\n{'entities': {'person': ['John Smith'],\n  'company': ['Acme Corp'],\n  'location': ['Berlin'],\n  'date': ['Jan 2021', 'late 2025']},\n 'category': 'employment'}\n\n\n\ngliner2(docs_to_classify[0])\n\n{'entities': {'person': ['John Smith'],\n  'company': ['Acme Corp'],\n  'location': ['Berlin'],\n  'date': ['late 2025', 'Jan 2021']},\n 'category': 'employment'}\n\n\n\nclassical_nlp(docs_to_classify[1])\n\n{'entities': {'person': [],\n  'company': [],\n  'location': ['Filadelphia'],\n  'date': []},\n 'category': 'weather'}\n\n\n\ngliner2(docs_to_classify[1])\n\n{'entities': {'person': [],\n  'company': [],\n  'location': ['Filadelphia'],\n  'date': []},\n 'category': 'weather'}\n\n\n\nclassical_nlp(docs_to_classify[2])\n\n{'entities': {'person': [],\n  'company': ['Anonumoys Inc.', 'NLP'],\n  'location': ['Warsaw', 'Poland'],\n  'date': ['February of 2026']},\n 'category': 'other'}\n\n\n\ngliner2(docs_to_classify[2])\n\n{'entities': {'person': [],\n  'company': ['Anonumoys Inc.'],\n  'location': ['Warsaw', 'Poland'],\n  'date': ['February of 2026']},\n 'category': 'other'}\n\n\n\n%%timeit\nclassical_nlp(docs_to_classify[2])\n\n727 ms ¬± 42.2 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\ngliner2(docs_to_classify[2])\n\n288 ms ¬± 29.5 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)"
  }
]